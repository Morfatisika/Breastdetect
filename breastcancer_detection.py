# -*- coding: utf-8 -*-
"""breastcancer detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dTt8Ukl60CVGQWRvNs830ZxGFBoBTNPb
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Define file path in shared Google Drive
csv_file_path = '/content/drive/MyDrive/data.csv'

"""importing the neccesarry libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

# Load the CSV file directly into a DataFrame
try:
    df = pd.read_csv(csv_file_path)
except pd.errors.ParserError:
    print("Error parsing CSV file. Please check the file format and delimiter.")

# Now df contains the DataFrame loaded from the shared Google Drive

df.head(100)

"""displaying 5 rows of the data and the columns names

"""

# Display the first few rows of the dataset
print(df.head())
print("______________________________________________\n")

#Display the column names
print("The columns in the data set are:")
df.columns

"""checking the information about the data"""

df.info()

df.describe().transpose()

# Display the distribution of the diagnosis
print(df['diagnosis'].value_counts())

# Check for missing values
print(df.isnull().sum())

# Check for duplicate rows
no_of_duplicates = df.duplicated().sum()
print(f'Number of duplicated data: {no_of_duplicates}')

"""distribution of classes.
from the plot below the data is somehow balanced

"""

sns.countplot(x='diagnosis', data=df)
plt.title('Distribution of Diagnosis')
plt.show()





"""encode the diagnosis column into"""

# Encode categorical variables
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

# Check if all columns are numeric
print(df.dtypes)

"""Pair Plot:
Visualize the relationship between pairs of features.
"""

sns.pairplot(df, vars=['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean'], hue='diagnosis')
plt.show()

from sklearn.preprocessing import StandardScaler

features = df.columns[2:]  # Exclude 'id' and 'diagnosis'
scaler = StandardScaler()
df[features] = scaler.fit_transform(df[features])

"""Correlation Matrix:

Identify correlations between features

Correlation Matrix Explanation

A correlation matrix is a table that shows the correlation coefficients between many variables.
Each cell in the table shows the correlation between two variables. The values range between -1 and 1:

1: Perfect positive correlation. When one variable increases, the other variable also increases proportionally.

0: No correlation. There is no relationship between the two variables.

-1: Perfect negative correlation. When one variable increases, the other variable decreases proportionally.

"""

corr_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""to drop id column from  the dataset"""

# Plot histograms for each variable
df.hist(figsize = (30, 30))
plt.show()

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Assuming df is your DataFrame with the breast cancer dataset
# df = pd.read_csv('breast_cancer_dataset.csv')

# Separate features (X) and target variable (y)
y = df['diagnosis']
X = df.drop(['diagnosis','id'],axis=1)

# Function to preprocess text or perform necessary data preprocessing steps
# Define this function based on your specific preprocessing needs
def preprocess_text(text):
    # Implement your preprocessing steps here if needed
    return text

# Function to train and test the model
def train_and_test_model(model, X, y):
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Create a pipeline with necessary preprocessing steps and the classifier
    pipeline_model = Pipeline([
        ('scaler', StandardScaler()),  # Example preprocessing step (you may adjust as needed)
        ('clf', model)  # Classifier model (e.g., RandomForestClassifier, SVM, etc.)
    ])

    # Train the pipeline model on the training data
    pipeline_model.fit(X_train, y_train)
    print('Accuracy on training data:', pipeline_model.score(X_train, y_train) * 100)

    # Print the accuracy on the test data
    print('Accuracy on testing data:', pipeline_model.score(X_test, y_test) * 100)

    # Generate predictions on the test data
    y_pred = pipeline_model.predict(X_test)

   # Print the classification report
    report = classification_report(y_test, y_pred)
    #print('Classification Report:\n', report)

    return pipeline_model, report

# Create a dictionary of models excluding MultinomialNB
models = {
    'SVC': SVC(C=1.0, kernel='rbf', gamma='scale'),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42)
}

# Train and test each model, and store the structured classification report
trained_models = {}
for model_name, model in models.items():
    print(f"Training and evaluating {model_name}...")
    pipeline_model, report = train_and_test_model(model, X, y)
    trained_models[model_name] = pipeline_model
    print(f"{model_name} Classification Report:\n {report}\n\n")

# Save the model to a file
import joblib
joblib.dump(pipeline_model, 'breast_cancer_model.pkl')

# Load the model from the file
loaded_model = joblib.load('breast_cancer_model.pkl')